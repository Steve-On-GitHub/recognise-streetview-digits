{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Recognise Streetview Digits\n",
    "=============\n",
    "\n",
    "\n",
    "3. Implementation\n",
    "------------\n",
    "\n",
    "- 3.1 Import modules and data\n",
    "- 3.2 Create helper functions\n",
    "- 3.3 Set up the model\n",
    "- 3.4 Run the model with different hyperparameters\n",
    "- 3.5 Show the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Import modules and data**\n",
    "\n",
    "Get the modules and load the data from the pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import gmtime, strftime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from extra_data_preprocessed_1_stretch.pickle.\n",
      "Data loaded from test_photos.pickle.\n",
      "Train data (85000, 64, 64, 1) float32\n",
      "Train labels (85000, 7) float64\n",
      "Test data (15000, 64, 64, 1) float32\n",
      "Test labels (15000, 7) float64\n",
      "Test photos (12, 64, 64) float32\n"
     ]
    }
   ],
   "source": [
    "def load_from_pickle(pickle_file):\n",
    "  with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "  print('Data loaded from %s.' % pickle_file)\n",
    "  return data\n",
    "\n",
    "# train_dataset, train_labels = load_from_pickle('train_data_preprocessed.pickle')\n",
    "# test_dataset,  test_labels  = load_from_pickle('test_data_preprocessed.pickle')\n",
    "extra_dataset_1, extra_labels_1 = load_from_pickle('extra_data_preprocessed_1_stretch.pickle')\n",
    "# extra_dataset_2, extra_labels_2 = load_from_pickle('extra_data_preprocessed_2.pickle')\n",
    "test_photos = load_from_pickle('test_photos.pickle')\n",
    "\n",
    "# to fit with memory constraints, we only use the first 100k 'extra' images for train and test.\n",
    "train_size = 85000 \n",
    "train_dataset, train_labels = extra_dataset_1[:train_size,:,:,:], extra_labels_1[:train_size,:]\n",
    "test_dataset,  test_labels  = extra_dataset_1[train_size:,:,:,:], extra_labels_1[train_size:,:]\n",
    "# train_dataset, train_labels = extra_dataset_1[:train_size,:,:], extra_labels_1[:train_size,:]\n",
    "# test_dataset,  test_labels  = extra_dataset_1[train_size:,:,:], extra_labels_1[train_size:,:]\n",
    "\n",
    "print('Train data',   train_dataset.shape, train_dataset.dtype)\n",
    "print('Train labels', train_labels.shape, train_labels.dtype)\n",
    "print('Test data',    test_dataset.shape, test_dataset.dtype)\n",
    "print('Test labels',  test_labels.shape, test_labels.dtype)\n",
    "print('Test photos',  test_photos.shape, test_photos.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat image data into a TensorFlow convolution-friendly shape (width by height by #channels). \n",
    "\n",
    "No need to reformat labels to 1-hot encodings (as we'll use sparse_softmax_cross_entropy_with_logits), but we need to remove the first label (the number length) given the simplified approach we're taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Data (85000, 64, 64, 1) float32\n",
      "Labels (85000, 5) int64\n",
      "[[    0 23759 16564 11454  8997  6927  5255  4995  3615  3434]\n",
      " [ 3921  8815  9170  8475  7541  8598  7203  7834  6468 16975]\n",
      " [34113  4850  5065  4873  4159  6319  4612  5033  4154 11822]\n",
      " [78965   596   531   557   511   749   532   583   461  1515]\n",
      " [84952     3     1     3     3     4     9     8     3    14]]\n",
      "Test data:\n",
      "Data (15000, 64, 64, 1) float32\n",
      "Labels (15000, 5) int64\n",
      "[[    0  4193  2998  2083  1570  1237   863   863   605   588]\n",
      " [  710  1480  1663  1501  1407  1433  1196  1403  1183  3024]\n",
      " [ 5903   863   900   893   737  1109   848   906   733  2108]\n",
      " [13880   119    93   100    87   146   106    98    90   281]\n",
      " [14988     1     1     1     2     3     0     0     2     2]]\n",
      "Test photos:\n",
      "Data (12, 64, 64, 1) float32\n"
     ]
    }
   ],
   "source": [
    "# Define global constants\n",
    "IMAGE_SIZE      = 64\n",
    "NUM_LABELS      = 11   # the logical size of the one-hots \n",
    "NUM_CLASSIFIERS =  5   # the number of softmax classifiers we'll create\n",
    "NUM_CHANNELS    =  1   # grayscale\n",
    "\n",
    "def reshape_images(images):  # reshape to have each pixel value in its own array, for the convolutions, if not already done\n",
    "  images = images.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "  print('Data', images.shape, images.dtype)\n",
    "  return images\n",
    "\n",
    "def reshape_labels(labels):  # ignore the length, just include the first n digits, where 0 means \"no digit\"  \n",
    "  labels = labels[:, 1:NUM_CLASSIFIERS+1].astype(np.int64)\n",
    "  print('Labels', labels.shape, labels.dtype)\n",
    "  print(np.array([np.histogram(label_i, bins=range(11))[0] for label_i in np.transpose(labels)], dtype='int'))\n",
    "  return labels\n",
    "\n",
    "print('Train data:')\n",
    "train_dataset, train_labels = reshape_images(train_dataset), reshape_labels(train_labels)\n",
    "print('Test data:')\n",
    "test_dataset, test_labels   = reshape_images(test_dataset),  reshape_labels(test_labels)\n",
    "print('Test photos:')\n",
    "test_photos = reshape_images(test_photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "**3.2 Create helper functions**\n",
    "\n",
    "First define the low level functions to create components of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "  input_data = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), name='Input/Data')\n",
    "  labels     = tf.placeholder(tf.int64, shape=(None, NUM_CLASSIFIERS), name='Input/Labels')\n",
    "  keep_prob  = tf.placeholder(tf.float32, name='Dropout_keep_prob')\n",
    "  tf.scalar_summary('Parameters/Dropout (keep)',keep_prob)\n",
    "  return input_data, labels, keep_prob\n",
    "\n",
    "def add_variable_summaries(variable, name): # To analyse all variables in tensorboard\n",
    "  if TENSORBOARD_SHOW_ALL:\n",
    "    with tf.name_scope('Summaries'):\n",
    "      tf.scalar_summary(name+'/min', tf.reduce_min(variable))\n",
    "      tf.scalar_summary(name+'/max', tf.reduce_max(variable))\n",
    "      mean = tf.reduce_mean(variable) # put it in a variable as we use it twice\n",
    "      with tf.name_scope('Std_dev'):\n",
    "        std_dev = tf.sqrt(tf.reduce_mean(tf.square(variable - mean)))\n",
    "      tf.scalar_summary(name+'/mean', mean)\n",
    "      tf.scalar_summary(name+'/std_dev', std_dev)\n",
    "      tf.histogram_summary(name, variable)\n",
    "\n",
    "def create_weights(layer_name, dimensions):\n",
    "  with tf.name_scope('Weights'):\n",
    "    input_dims = np.prod(dimensions[:-1])   # the last element is the output dimension, all others inputs\n",
    "    init_stddev = np.sqrt(2.0 / input_dims) # this ensures the variance after ReLU is 1\n",
    "    weights = tf.Variable(tf.truncated_normal(dimensions,stddev = init_stddev))\n",
    "    add_variable_summaries(weights, layer_name+'/weights')\n",
    "  return weights\n",
    "\n",
    "def create_biases(layer_name, output_dim):\n",
    "  with tf.name_scope('Biases'):\n",
    "    biases  = tf.Variable(tf.zeros([output_dim])) # initialise to zero\n",
    "    add_variable_summaries(biases, layer_name+'/biases')\n",
    "  return biases\n",
    "\n",
    "def create_activations(layer_name, pre_activations, activation_func):\n",
    "  with tf.name_scope('Activations'):\n",
    "    activations = activation_func(pre_activations)\n",
    "    tf.histogram_summary(layer_name+'/2_activations', activations)\n",
    "  return activations\n",
    "\n",
    "def add_l2_losses(layer_name, l2_loss_list, weights, biases):\n",
    "  with tf.name_scope('L2_losses'):\n",
    "    l2_losses = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    tf.scalar_summary(layer_name+'/L2_losses',l2_losses)\n",
    "    l2_loss_list.append(l2_losses)\n",
    "    return l2_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the functions to build the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_conv_layer(layer_name, input_tensor, l2_loss_in, patch_size, out_channels, stride, \n",
    "                      max_pool_kernel, max_pool_stride, activation_func):\n",
    "  in_channels = input_tensor.get_shape().as_list()[3]\n",
    "  with tf.name_scope(layer_name):\n",
    "    weights = create_weights(layer_name, [patch_size, patch_size, in_channels, out_channels])\n",
    "    biases  = create_biases(layer_name, out_channels)\n",
    "    with tf.name_scope('Convolutions'):\n",
    "      strides = [1, stride, stride, 1]\n",
    "      convolutions = tf.nn.conv2d(input_tensor, weights, strides, padding=\"SAME\")\n",
    "      tf.histogram_summary(layer_name+'/1_convolutions', convolutions)\n",
    "    with tf.name_scope('Max_pooling'):\n",
    "      if max_pool_stride > 1:\n",
    "        ksize   = [1, max_pool_kernel, max_pool_kernel, 1]\n",
    "        strides = [1, max_pool_stride, max_pool_stride, 1]\n",
    "        convolutions = tf.nn.max_pool(convolutions, ksize, strides, padding=\"SAME\")\n",
    "    activations = create_activations(layer_name, convolutions + biases, activation_func)\n",
    "    l2_loss_out = add_l2_losses(layer_name, l2_loss_in, weights, biases)\n",
    "  return activations, l2_loss_out, weights\n",
    "\n",
    "def create_all_conv_layers(next_input, l2_loss, conv_layers, keep_prob):\n",
    "  weights_list = [] # return the weights for visualisation\n",
    "  for i, conv_layer in enumerate(conv_layers):\n",
    "    patch_size, depth, stride, mp_kernel, mp_stride = conv_layer\n",
    "    next_input, l2_loss, weights = create_conv_layer('Convolution'+str(i+1), next_input, l2_loss, \n",
    "                                            patch_size, depth, stride, mp_kernel, mp_stride, tf.nn.relu) \n",
    "    next_input = tf.nn.dropout(next_input, keep_prob, name='Dropout')\n",
    "    weights_list.append(weights)\n",
    "  return next_input, l2_loss, weights_list\n",
    "\n",
    "def flatten_conv_to_fc(next_input):\n",
    "  with tf.name_scope('Flatten'):\n",
    "    shape      = next_input.get_shape().as_list()\n",
    "    next_input = tf.reshape(next_input, [-1,shape[1]*shape[2]*shape[3]])\n",
    "  return next_input\n",
    "\n",
    "def create_fully_connected_layer(layer_name, input_tensor, l2_loss, input_dim, output_dim, activation_func):\n",
    "  with tf.name_scope(layer_name):\n",
    "    weights = create_weights(layer_name, [input_dim, output_dim])\n",
    "    biases  = create_biases(layer_name, output_dim)\n",
    "    with tf.name_scope('XW_plus_b'):\n",
    "      pre_activations = tf.matmul(input_tensor, weights) + biases\n",
    "      tf.histogram_summary(layer_name+'/1_pre_activations', pre_activations)\n",
    "    activations = create_activations(layer_name, pre_activations, activation_func)\n",
    "    l2_loss = add_l2_losses(layer_name, l2_loss, weights, biases)\n",
    "  return activations, l2_loss\n",
    "\n",
    "def create_all_fc_layers(next_input, l2_loss, fc_layers, keep_prob):\n",
    "  fc_layer_count = len(fc_layers)\n",
    "  input_dims     = next_input.get_shape().as_list()[1]\n",
    "  fc_node_counts = [input_dims] + fc_layers\n",
    "  for i in range(fc_layer_count):\n",
    "    next_input, l2_loss = create_fully_connected_layer('Fully_Connected'+str(i+1), next_input, l2_loss,\n",
    "                                                       fc_node_counts[i],fc_node_counts[i+1],\n",
    "                                                       tf.nn.relu if i+1 < fc_layer_count else tf.identity) \n",
    "    next_input = tf.nn.dropout(next_input, keep_prob, name='Dropout') if i+1 < fc_layer_count else next_input\n",
    "  return next_input, l2_loss \n",
    "\n",
    "def create_logits_for_all_outputs(features, l2_loss):\n",
    "  input_dim = features.get_shape().as_list()[1]\n",
    "  logits_list = []\n",
    "  for i in range(NUM_CLASSIFIERS):\n",
    "    layer_name = 'Digit_'+str(i+1)\n",
    "    logits, l2_loss = create_fully_connected_layer(layer_name, features, l2_loss, input_dim, NUM_LABELS, tf.identity)\n",
    "    logits_list.append(logits)\n",
    "  return logits_list, l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, define the functions to calculate loss and accuracy, create the training step, and any helper functions to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(logits_list, labels, l2_loss_list, weight_decay, batch_size):\n",
    "  with tf.name_scope('Loss'):\n",
    "    with tf.name_scope('Cross_entropy'):\n",
    "      cross_entropy_total = 0.0\n",
    "      cross_entropy_list = []                 # create this list to return for debugging \n",
    "      labels_list = tf.unpack(labels, axis=1) # unpack to have one loss object per classifier in the graph\n",
    "      for i in range(NUM_CLASSIFIERS):\n",
    "        name            = 'Digit_'+str(i+1)\n",
    "        cross_entropies = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_list[i], labels_list[i])\n",
    "        cross_entropy_list.append(cross_entropies) \n",
    "        cross_entropy   = tf.reduce_mean(cross_entropies, name=name)\n",
    "        tf.scalar_summary('Loss/Cross_entropy/'+name, cross_entropy)\n",
    "        cross_entropy_total += cross_entropy\n",
    "      tf.scalar_summary('Loss/Cross_entropy/Total', cross_entropy_total)\n",
    "    with tf.name_scope('L2_loss'):\n",
    "      l2_loss = tf.add_n(l2_loss_list)\n",
    "      l2_loss_component = weight_decay * batch_size / TRAIN_SIZE * l2_loss\n",
    "      tf.scalar_summary('Parameters/Weight_decay',weight_decay)\n",
    "      tf.scalar_summary('Loss/L2_loss_component',l2_loss_component)\n",
    "    loss = cross_entropy_total + l2_loss_component\n",
    "    tf.scalar_summary('Loss/Total',loss)\n",
    "  return loss, cross_entropy_list\n",
    "\n",
    "def calculate_accuracy(logits_list, labels):\n",
    "  with tf.name_scope('Accuracy'):\n",
    "    with tf.name_scope('Predictions'):\n",
    "      logits_array = tf.pack(logits_list, axis=1) # put the logits into a big array: [samples, classifiers, labels]\n",
    "      predictions  = tf.argmax(logits_array, 2)   # find the label with the highest logit (softmax is monotonic)\n",
    "    with tf.name_scope('Accuracy'):\n",
    "      correct_predictions = tf.reduce_all(tf.equal(predictions, labels),1) # sample is correct if all classifiers are correct \n",
    "      accuracy            = 100.0 * tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    tf.scalar_summary('Accuracy',accuracy)\n",
    "  return accuracy\n",
    "\n",
    "def create_training_step(loss, initial_rate, learn_decay_steps, learn_decay_rate, algorithm, global_step):\n",
    "  with tf.name_scope('Train'):\n",
    "    if algorithm == 'Adam': \n",
    "      optimizer = tf.train.AdamOptimizer()\n",
    "    else:\n",
    "      with tf.name_scope('Learning_rate'):\n",
    "        learning_rate = tf.train.exponential_decay(initial_rate, global_step, learn_decay_steps, learn_decay_rate)\n",
    "        tf.scalar_summary('Parameters/Learning rate',learning_rate)\n",
    "      if algorithm == 'AdLR': \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "      elif algorithm[0]  == 'M': \n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, float(algorithm[1:]))  # assume \"M0.5\" for momentum 0.5 \n",
    "      else: \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_step\n",
    "\n",
    "def get_next_batch(step, batch_size, train_dataset, train_labels):\n",
    "  offset = (step * batch_size) % (TRAIN_SIZE - batch_size) \n",
    "  batch_data   = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "  batch_labels =  train_labels[offset:(offset + batch_size), :]\n",
    "  return batch_data, batch_labels\n",
    "\n",
    "def log_bucket_delta(step, num_log_buckets, num_steps):\n",
    "  return (step + 1) * num_log_buckets // num_steps - step * num_log_buckets // num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Set up the model **\n",
    "\n",
    "Using the helper functions, create the graph according to the specified design and regularisation hyperparameters. Then create the loop to run it according to the specified batch size, iterations and logging parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_and_run_model(data, params, log_params):\n",
    "  # unpack the arguments\n",
    "  train_dataset, train_labels, \\\n",
    "    test_dataset, test_labels, test_photos                = data\n",
    "  num_steps, batch_size, (conv_layers, _), fc_layers,\\\n",
    "    initial_rate, learn_decay_rate, learn_decay_steps, \\\n",
    "    weight_decay, algorithm, dropout, nickname            = params\n",
    "  log_dir, log_printouts, tb_printouts, tb_show_all       = log_params\n",
    "  \n",
    "  # set up logging parameters\n",
    "  do_logging     = log_printouts > 0\n",
    "  do_tensorboard = tb_printouts  > 0\n",
    "  log_printouts  = min(num_steps, log_printouts) - 1 # can't log more times than we have steps, ... \n",
    "  tb_print_steps = min(num_steps, tb_printouts)  - 1 # ... and subtract 1 because the first step is always logged.\n",
    "  \n",
    "  # set up network constants\n",
    "  global TRAIN_SIZE, IMAGE_SIZE, NUM_CHANNELS, NUM_LABELS, NUM_CLASSIFIERS, TENSORBOARD_SHOW_ALL\n",
    "  TENSORBOARD_SHOW_ALL = tb_show_all\n",
    "  TRAIN_SIZE = train_dataset.shape[0]\n",
    "  graph = tf.Graph()\n",
    "\n",
    "  # define the graph\n",
    "  with graph.as_default():\n",
    "   \n",
    "    # create placeholders and variables\n",
    "    input_data, labels, keep_prob = create_placeholders()\n",
    "    global_step = tf.Variable(0, trainable=False, name='Global_step')\n",
    "\n",
    "    # build the network\n",
    "    next_input,  l2_loss = input_data, []\n",
    "    next_input,  l2_loss, conv_weights = create_all_conv_layers(next_input, l2_loss, conv_layers, keep_prob)\n",
    "    next_input           = flatten_conv_to_fc(next_input)\n",
    "    features,    l2_loss = create_all_fc_layers(next_input, l2_loss, fc_layers, keep_prob)\n",
    "    logits_list, l2_loss = create_logits_for_all_outputs(features, l2_loss)\n",
    "    \n",
    "    # calculate loss and accuracy; set up the training step\n",
    "    loss, loss_list = calculate_loss(logits_list, labels, l2_loss, weight_decay, batch_size)\n",
    "    accuracy        = calculate_accuracy(logits_list, labels)\n",
    "    train_step      = create_training_step(loss, initial_rate, learn_decay_steps, learn_decay_rate, algorithm, global_step)\n",
    "\n",
    "    # bring together the summaries, create writers for them\n",
    "    merged = tf.merge_all_summaries()\n",
    "\n",
    "  # now run the graph\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    # initialise the summary writers\n",
    "    train_writer = tf.train.SummaryWriter(log_dir+'/train', graph)\n",
    "    test_writer  = tf.train.SummaryWriter(log_dir+'/test')\n",
    "\n",
    "    # initialise log and timer for the homemade plotter\n",
    "    learning_log = np.array([]).reshape(0,5) # the array we'll use to plot the learning curve\n",
    "    start = timer()\n",
    "\n",
    "    # the test and photo feed dictionary doesn't change so define it now\n",
    "    test_feed_dict  = {input_data : test_dataset, labels : test_labels, keep_prob : 1.0}\n",
    "    photo_feed_dict = {input_data : test_photos, keep_prob : 1.0}\n",
    "  \n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "      # define the feed dictionary for the training run - this happens every step\n",
    "      batch_data, batch_labels = get_next_batch(step, batch_size, train_dataset, train_labels)\n",
    "      train_feed_dict = {input_data : batch_data, labels : batch_labels, keep_prob : dropout}\n",
    "\n",
    "      # sometimes we'll calculate test accuracy and save to tensorboard\n",
    "      if do_tensorboard and (step == 0 or log_bucket_delta(step, tb_printouts, num_steps) > 0):\n",
    "        _, train_loss, train_acc, train_summary = sess.run([train_step, loss, accuracy, merged], feed_dict=train_feed_dict)\n",
    "        test_loss,      test_acc,  test_summary = sess.run([            loss, accuracy, merged], feed_dict=test_feed_dict)\n",
    "        train_writer.add_summary(train_summary, step)\n",
    "        test_writer.add_summary( test_summary,  step)\n",
    "\n",
    "      # sometime we'll just calculate test accuracy for the homemade plotter\n",
    "      elif do_logging and (step == 0 or log_bucket_delta(step, log_printouts, num_steps) > 0):\n",
    "        _, train_loss, train_acc = sess.run([train_step, loss, accuracy], feed_dict=train_feed_dict)\n",
    "        test_loss, test_acc      = sess.run([            loss, accuracy], feed_dict=test_feed_dict)\n",
    "\n",
    "      # and otherwise, we just train the network\n",
    "      else: sess.run([train_step], feed_dict=train_feed_dict)\n",
    "\n",
    "      if do_logging and log_bucket_delta(step, log_printouts, num_steps) > 0 :\n",
    "        learning_log = np.append(learning_log, [[step, train_loss, test_loss, train_acc, test_acc]], axis=0)\n",
    "        \n",
    "      dots_to_print = log_bucket_delta(step, 100, num_steps)\n",
    "      if dots_to_print > 0: print('.'*dots_to_print, end='')\n",
    "\n",
    "    # Finally, close the writers,  calculate accuracy on the test data:\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    test_accuracy, test_logits, test_losses, c_weights = sess.run([accuracy, logits_list, loss_list, conv_weights],\n",
    "                                                        feed_dict=test_feed_dict)\n",
    "    photo_logits = sess.run([logits_list], feed_dict = photo_feed_dict)\n",
    "    run_time     = timer() - start\n",
    "    output_data  = test_logits, test_losses, photo_logits, c_weights\n",
    "  return learning_log, run_time, test_accuracy, output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Run the model with different hyperparameters**\n",
    "\n",
    "First, set up any helper functions for the logging, and load the saved results log and set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results log loaded from results.pickle\n"
     ]
    }
   ],
   "source": [
    "run_text_header = 'run  step btc conv_layers  fc_layers   learnR  ldr  lds     wd   alg d/o Nickname         - t_acc min'\n",
    "     # e.g.        85: 10000 123 cl_16_32_128 [1024]        0.05 0.95 1000  0.001 M0.05 1.0 L_r_decay 0.9/k  - 87.4%   8\n",
    "     # sizes        2:     5   3 12           11               6    4    4      6     5   3 16               -    4%   3\n",
    "\n",
    "def run_text_long(params, run_counter, test_accuracy, run_time): # get the whole config printable to one line\n",
    "  st, b, (_, conv_l), fc_l, lr, ldr, lds, wd, l_a, do, Nickname = params\n",
    "  return '{:>2d}: {:>5d} {:>3d} {:<12} {:<11} {:>6} {:>4} {:>4} {:>6} {:>5} {:>3.1f} {:<16} - {:>4.1f}% {:>3.0f}'.format(\n",
    "          run_counter, st, b,    conv_l, fc_l,   lr,   ldr, lds,   wd,  l_a,  do, Nickname, test_accuracy, run_time/60)\n",
    "\n",
    "def run_text_short(results):\n",
    "  i, params, _, _, test_accuracy, run_time, _ = results\n",
    "  return '{:>2d}: {:<20} - {:>4.1f}% acc in {:>4.0f}m'.format(i, params[-1], test_accuracy, run_time/60)\n",
    "  \n",
    "def load_results_log(filename):\n",
    "  if os.path.exists(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      results_log = pickle.load(f)\n",
    "    run_counter = len(results_log) \n",
    "    print('Results log loaded from {}'.format(filename))\n",
    "  else:\n",
    "    results_log, run_counter = [],0\n",
    "    print('Results log initialised')\n",
    "  return results_log, run_counter\n",
    "\n",
    "def save_results_log(data, filename):\n",
    "  pickle_file = filename\n",
    "  os.environ['TZ'] = 'Europe/Zurich'\n",
    "  time.tzset()\n",
    "  try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"{} saved at {}\".format(filename,strftime(\"%H:%M\")))\n",
    "    f.close()\n",
    "  except Exception as e:\n",
    "    print('Unable to save data to', filename, ':', e)\n",
    "    raise\n",
    "\n",
    "if 'results_log' not in locals() or True:\n",
    "  results_log, run_counter = load_results_log(\"results.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify a list of hyperparameter combinations and run the model on those, adding the results to the results log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  step btc conv_layers  fc_layers   learnR  ldr  lds     wd   alg d/o Nickname         - t_acc min\n",
      "    |0        |10       |20       |30       |40       |50       |60       |70       |80       |90       |100\n",
      "23: .....................................................................................................\n",
      "23: 10000 128 c4_16>128s12 [256, 128]   0.001 0.92  500   0.01  AdLR 0.9 Final model      - 91.3% 541\n",
      "results.pickle saved at 07:18\n"
     ]
    }
   ],
   "source": [
    "########################## Set the hyperparameters to test #############################\n",
    "# conv_layers = ( [(patch_size, depth, stride, mp_kernel, mp_stride), ...] , name)     #\n",
    "# params = 0 steps, 1 batch, 2 convolution_layers, 3 fc_layers,                        #\n",
    "#          4 learn_rate, 5 ldecay_rate, 6 ldecay_steps,                                #\n",
    "#          7 weight_decay, 8 learning algorithm, 9 dropout, 10 Nickname                #\n",
    "########################################################################################\n",
    "\n",
    "def define_conv_layers(node_list, stride_list, name=None):\n",
    "  layer = [(5, nodes, stride, 2, 2) for nodes, stride in zip(node_list, stride_list)]\n",
    "  if name is None: name = 'cl_'+'_'.join(str(node) for node in node_list)\n",
    "  return (layer, name)\n",
    "\n",
    "c2_16_32s2   = define_conv_layers([16,32],        [2,2],     'c2_16>32s2')\n",
    "c3_16_64s121 = define_conv_layers([16,32,64],     [1,2,1],   'c3_16>64s121')\n",
    "c3_16_64s212 = define_conv_layers([16,32,64],     [2,1,2],   'c3_16>64s212')\n",
    "c3_32_64s121 = define_conv_layers([32,32,64],     [1,2,1],   'c3_32>64s121')\n",
    "c3_32_64s212 = define_conv_layers([32,32,64],     [2,1,2],   'c3_32>64s212')\n",
    "c4_16_128s12 = define_conv_layers([16,32,64,128], [1,1,1,2], 'c4_16>128s12')\n",
    "\n",
    "#  0 st, 1 b,    2 conv_l,      3 fc_l,   4 lr, 5 ldr, 6 lds,  7 wd,  8 l_a, 9 do, 10 Nickname     |\n",
    "param_list = [ \\\n",
    " (10000, 128,c4_16_128s12,  [256, 128],   0.001 ,  0.92,   500, 0.01 , \"AdLR\",  0.9, \"Final model\")]\n",
    "\n",
    "log_printouts =   100            # How many times to write validation stats to the log file \n",
    "tb_printouts  =    20            # How many times to write train & validation stats to tensorboard\n",
    "tb_show_all   = False            # Whether to log stats on the weights and biases\n",
    "log_path      = \"tb/optimise/\"   # Where to save the tensorboard logs\n",
    "progress_bar  = '    |' + '|'.join('{:<2}       '.format(str(i*10)) for i in range(10)) + '|100'\n",
    "data          = (train_dataset, train_labels, test_dataset, test_labels, test_photos)\n",
    "\n",
    "print(run_text_header)\n",
    "print(progress_bar)\n",
    "\n",
    "##### Run the model and log the results #####\n",
    "for params in param_list:\n",
    "  log_dir    = log_path + str(run_counter) + \"_\" + params[-1]\n",
    "  log_params = (log_dir, log_printouts, tb_printouts, tb_show_all)\n",
    "\n",
    "  print('{:>2}: .'.format(run_counter), end='')\n",
    "  learning_log, run_time, test_accuracy, output_data = define_and_run_model(data, params, log_params)\n",
    "  run_text = run_text_long(params, run_counter, test_accuracy, run_time)\n",
    "  print('\\n' + run_text)\n",
    "  results_log.append((run_counter, params, run_text, learning_log, test_accuracy, run_time, output_data))\n",
    "  run_counter += 1\n",
    "\n",
    "save_results_log(results_log, 'results.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Show the results**\n",
    "\n",
    "Show the results in detail in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard 28 on port 8889\n",
      "(You can navigate to http://172.17.0.2:8889)\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n"
     ]
    }
   ],
   "source": [
    "# !sudo netstat -ap | grep :8889    # in case Tensorboard hangs, use this to find the process to kill...\n",
    "# !kill 1014                        # ... and kill it.\n",
    "!tensorboard --port 8889 --logdir tb/optimise/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the runs saved to the results log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  step btc conv_layers  fc_layers   learnR  ldr  lds     wd   alg d/o Nickname         - t_acc min\n",
      " 0: 10000 128 c2_16>32s2   [1024]        None  1.0    1    0.0  Adam 1.0 Baseline         - 48.3%  60\n",
      " 1: 10000 128 c2_16>32s2   [1024]       0.001  1.0    1    0.0  AdLR 1.0 AdamLR=0.001     - 49.2%  60\n",
      " 2: 10000 128 c2_16>32s2   [1024]      0.0001  1.0    1    0.0  AdLR 1.0 AdamLR=0.0001    - 31.9%  59\n",
      " 3: 10000 128 c2_16>32s2   [1024]       0.001  0.9 1000    0.0  AdLR 1.0 AdamLRD 0.9/k    - 48.1%  61\n",
      " 4: 10000 128 c2_16>32s2   [256]         None  1.0    1    0.0  Adam 1.0 Only 256FCnodes  - 49.0%  55\n",
      " 5: 10000 128 c2_16>32s2   [256]        0.001 0.97  100    0.0  AdLR 1.0 LRD 0.97/100     - 45.3%  56\n",
      " 6: 10000 128 c2_16>32s2   [256]         None  1.0    1   0.01  Adam 1.0 W_decay 0.01     - 47.8%  54\n",
      " 7: 10000 128 c2_16>32s2   [256]         None  1.0    1  0.001  Adam 1.0 W_decay 0.001    - 48.3%  54\n",
      " 8: 10000 128 c2_16>32s2   [256, 64]     None  1.0    1    0.0  Adam 1.0 2 FC layers      - 54.5%  55\n",
      " 9: 10000 128 c3_16>64s121 [512]         None  1.0    1    0.0  Adam 1.0 3CLs 512FCnodes  - 64.5% 244\n",
      "10:  5000 128 c3_16>64s121 [256]         None  1.0    1    0.0  Adam 1.0 3CLs 256FCnodes  - 64.8% 120\n",
      "11: 10000 128 c3_16>64s212 [256]         None  1.0    1    0.0  Adam 1.0 Stride 2,1,2     - 61.6%  98\n",
      "12: 10000 128 c3_32>64s212 [256]         None  1.0    1    0.0  Adam 1.0 CL1 depth 32     - 61.3% 162\n",
      "13: 10000 128 c3_32>64s212 [256, 128]    None  1.0    1    0.0  Adam 1.0 2FC: 256,128     - 64.5% 167\n",
      "14: 10000 128 c4_16>128s12 [256]         None  1.0    1    0.0  Adam 1.0 2 CLs, s1112     - 75.7% 567\n",
      "15: 10000 128 c3_16>64s212 [256]         None  1.0    1    0.0  Adam 0.5 S212+Dropout0.5  - 32.9%  99\n",
      "16: 10000 128 c3_16>64s212 [256]         None  1.0    1   0.01  Adam 1.0 ..W_decay 0.01   - 62.2%  98\n",
      "17: 10000 128 c3_16>64s212 [256]         None  1.0    1  0.001  Adam 1.0 ..W_decay 0.001  - 61.9%  99\n",
      "18: 10000 128 c3_16>64s212 [256]        0.001 0.92  500    0.0  AdLR 1.0 ..LRD 0.92/500   - 62.0%  98\n",
      "19: 10000 128 c4_16>128s12 [256, 128]   0.001 0.92  500   0.01  AdLR 0.9 4+2L DO WD LRD   - 86.8% 542\n",
      "21: 10000 128 c4_16>128s12 [256, 128]   0.001 0.92  500   0.01  AdLR 0.9 Stretch not pad  - 91.7% 546\n",
      "21: 10000 128 c4_16>128s12 [256, 128]   0.001 0.92  500   0.01   SGD 0.9 Stretch+SGD      - 18.5% 544\n",
      "22:  2000 128 c4_16>128s12 [256, 128]    0.01 0.92  500   0.01   SGD 0.9 SGD, LR 0.01     - 60.1% 256\n",
      "23: 10000 128 c4_16>128s12 [256, 128]   0.001 0.92  500   0.01  AdLR 0.9 Final model      - 91.3% 541\n"
     ]
    }
   ],
   "source": [
    "print(run_text_header)\n",
    "for n,results in enumerate(results_log): \n",
    "  i, params, run_text, learning_log, test_accuracy, run_time, output_data = results\n",
    "  print(run_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
